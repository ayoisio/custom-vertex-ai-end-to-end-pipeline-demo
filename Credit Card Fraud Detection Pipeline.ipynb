{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e103b8-37c8-4bfb-be60-7461aa43d50e",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection Pipeline\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/513/1*aeXlwnOS3DvVHiMVgBZbpQ.png?raw=1\" alt=\"Vertex AI Logo\" align=\"left\" height=\"50\" width=\"50\" />\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Tensorflow_logo.svg/1200px-Tensorflow_logo.svg.png?raw=1\" alt=\"Vertex AI Logo\" align=\"left\" height=\"50\" width=\"50\" style=\"margin-left:20px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a66dde-beb5-4d62-8c65-f9101505a2c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1085125d-93e3-4b5a-b5f9-2decc4dcbef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7648794b-8234-4301-8fdd-a8de21b47d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install {USER_FLAG} google-cloud-aiplatform==1.18.3 --upgrade\n",
    "# !pip3 install {USER_FLAG} kfp==1.8.16 google-cloud-pipeline-components==1.0.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ff2cdb-0721-42fd-b781-00434a33f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "# !python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b79a-0eba-42a1-8e8e-b45499ccb823",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0040cabc-6857-499d-a6f4-323f0ae50119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google_cloud_pipeline_components.experimental.evaluation as gcc_evaluation\n",
    "import json\n",
    "import kfp\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component, pipeline, Artifact, Input, Output, Model, Metrics, ClassificationMetrics\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components.experimental.vertex_notification_email import VertexNotificationEmailOp\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b8ce0-fcf9-46a5-9f6a-daf248121fdc",
   "metadata": {},
   "source": [
    "## Set project ID and bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd37f6c0-6771-4cf6-a4ab-3dc42b0e9f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID:  aadev-end-to-end-pipeline-demo\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"PROJECT_ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe283e94-67cc-4606-b32e-3c1aa1261037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_NAME: gs://demo-kfp-pipelines\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = os.path.join(\"gs://\", \"demo-kfp-pipelines\")\n",
    "print(\"BUCKET_NAME:\", BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79cee8-c5d4-4ce9-89e9-ec7c5b9d0f01",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066d4201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/Users/ayoad/Downloads/cvs-end-to-end-pipeline-demo/venv/bin:/usr/local/opt/openjdk@11/bin:/Users/ayoad/Downloads/google-cloud-sdk/bin:/opt/anaconda3/bin:/opt/anaconda3/condabin:/usr/local/git/git-google/bin:/usr/local/git/current/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de47972c-de47-4b7e-80db-d74cdb0c7f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT: gs://demo-kfp-pipelines/pipeline-root\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_ROOT = os.path.join(BUCKET_NAME, \"pipeline-root\")\n",
    "print(\"PIPELINE_ROOT:\", PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bbfe4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKDIR: gs://demo-kfp-pipelines/pipeline-root/1668969721\n"
     ]
    }
   ],
   "source": [
    "UUID = str(int(time.time()))\n",
    "WORKDIR = os.path.join(PIPELINE_ROOT, UUID)\n",
    "print(f'WORKDIR: {WORKDIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c4d826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_NOTIFICATION_RECIPIENTS = [\"ayoad@google.com\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85059391-f7fb-4652-b24b-95e9f925a2da",
   "metadata": {},
   "source": [
    "## Create component for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e1bb8b-edac-45e8-9483-9bbcf7464422",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def make_deployment_decision(project: str, staging_bucket: str, metric_name: str, threshold: float) -> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):\n",
    "    import json\n",
    "    import logging\n",
    "    import os\n",
    "    import re\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # determine bucket name and prefix\n",
    "    staging_bucket_components = staging_bucket.split(\"gs://\")[-1].split(\"/\")\n",
    "    bucket_name = staging_bucket_components[0]\n",
    "    prefix = os.path.join(*staging_bucket_components[1:])\n",
    "    \n",
    "    # determine evaluation metrics output file \"executor_output.json\" path\n",
    "    storage_client = storage.Client(project=project)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blobs = storage_client.list_blobs(bucket, prefix=prefix)\n",
    "    regex = re.compile(r'.*/model-evaluation-classification_.*/executor_output.json')\n",
    "    matching_metrics_blobs = list(filter(regex.match, [blob.name for blob in blobs]))\n",
    "    evaluation_metrics_blob_name = matching_metrics_blobs[0]\n",
    "    \n",
    "    # read blob and create results dict\n",
    "    blob = bucket.blob(evaluation_metrics_blob_name)\n",
    "    metrics_data_dict = json.loads(blob.download_as_bytes(client=storage_client))\n",
    "    results_dict = metrics_data_dict[\"artifacts\"][\"evaluation_metrics\"][\"artifacts\"][0]['metadata']\n",
    "   \n",
    "    # determine metric value\n",
    "    metric_value = results_dict[metric_name]\n",
    "    \n",
    "    if metric_value > threshold:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "        \n",
    "    logging.info(\"deployment decision is %s\", dep_decision)\n",
    "\n",
    "    return (dep_decision,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd7d97-21ee-4bff-b111-d19ef91b766e",
   "metadata": {},
   "source": [
    "## Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6987cc45-bd32-4bb7-aedf-a0005ae6f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"credit-card-fraud-detection-training\", pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "    dataset_display_name: str = \"credit_card_fraud_detection\",\n",
    "    bq_source: str = \"bq://aadev-end-to-end-pipeline-demo.gcs_lake.credit_card_fraud_detection\",\n",
    "    training_display_name: str = \"credit-card-fraud-detection-tf-train\",\n",
    "    staging_bucket: str = WORKDIR,\n",
    "    container_uri: str = \"us-central1-docker.pkg.dev/aadev-end-to-end-pipeline-demo/vertex-ai-pipeline-custom-training-jobs/credit_card_fraud_detection_tf:latest\",\n",
    "    target_column_name: str = \"Class\",\n",
    "    class_names: list = [\"0\", \"1\"],\n",
    "    key_columns: list = [\"key\"],\n",
    "    bq_dest: str = f\"bq://{PROJECT_ID}\",\n",
    "    model_display_name: str = \"credit-card-fraud-detection-tf\",\n",
    "    model_serving_container_image_uri: str = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-9:latest\",\n",
    "    training_machine_type: str = \"n1-standard-4\",\n",
    "    batch_predict_display_name: str = \"credit-card-fraud-detection-tf-batch-predict\",\n",
    "    batch_predict_machine_type: str = \"n1-standard-4\",\n",
    "    deployment_metric: str = \"auPrc\",\n",
    "    deployment_threshold: float = 0.95,\n",
    "    aiplatform_host: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "    endpoint_display_name: str = \"credit-card-fraud-detection-tf\",\n",
    "    dedicated_resources_min_replica_count: int = 1,\n",
    "    dedicated_resources_max_replica_count: int = 1,\n",
    "    dedicated_resources_machine_type: str = \"n1-standard-4\"\n",
    "):\n",
    "    notify_email_task = VertexNotificationEmailOp(recipients=EMAIL_NOTIFICATION_RECIPIENTS)\n",
    "\n",
    "    with dsl.ExitHandler(notify_email_task):\n",
    "        \n",
    "        # create dataset\n",
    "        dataset_create_task = gcc_aip.TabularDatasetCreateOp(\n",
    "            project=project,\n",
    "            location=gcp_region,\n",
    "            display_name=dataset_display_name, \n",
    "            bq_source=bq_source\n",
    "        )\n",
    "\n",
    "        # determine test dataset file path\n",
    "        test_dataset_file_path = os.path.join(WORKDIR, \"batch-prediction\", \"input\", \"test_dataset.jsonl\")\n",
    "        \n",
    "        # determine splitter output path\n",
    "        splitter_output_path = os.path.join(WORKDIR, \"batch-prediction\", \"splitter\")\n",
    "        \n",
    "        # determine batch prediction output path\n",
    "        batch_prediction_output_path = os.path.join(WORKDIR, \"batch-prediction\", \"output\")\n",
    "\n",
    "        # create custon container training job\n",
    "        training_task = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "            display_name=training_display_name,\n",
    "            container_uri=container_uri,\n",
    "            project=project,\n",
    "            location=gcp_region,\n",
    "            dataset=dataset_create_task.outputs[\"dataset\"],\n",
    "            staging_bucket=staging_bucket,\n",
    "            bigquery_destination=bq_dest,\n",
    "            environment_variables={\n",
    "                \"TEST_DATASET_FILE_PATH\": test_dataset_file_path,\n",
    "                \"PYTHONUNBUFFERED\": \"1\",\n",
    "                \"CLASS_COLUMN\": \"Class\",\n",
    "                \"REMOVE_INPUT_COLUMNS\": json.dumps([\"Time\", \"Row_Weight\"])\n",
    "            },\n",
    "            model_serving_container_image_uri=model_serving_container_image_uri,\n",
    "            model_display_name=model_display_name,\n",
    "            machine_type=training_machine_type,\n",
    "        )\n",
    "\n",
    "        # remove target column from batch prediction input path\n",
    "        target_remover_task = gcc_evaluation.TargetFieldDataRemoverOp(\n",
    "            project=project,\n",
    "            location=gcp_region,\n",
    "            root_dir=splitter_output_path,\n",
    "            gcs_source_uris=[test_dataset_file_path],\n",
    "            instances_format=\"jsonl\",\n",
    "            target_field_name=target_column_name,\n",
    "        ).after(training_task)\n",
    "\n",
    "        # make batch prediction\n",
    "        batch_prediction_task = gcc_aip.ModelBatchPredictOp(\n",
    "            project=project,\n",
    "            location=gcp_region,\n",
    "            job_display_name=batch_predict_display_name,\n",
    "            model=training_task.outputs[\"model\"],\n",
    "            gcs_source_uris=target_remover_task.outputs[\"gcs_output_directory\"],\n",
    "            instances_format=\"jsonl\",\n",
    "            predictions_format=\"jsonl\",\n",
    "            gcs_destination_output_uri_prefix=batch_prediction_output_path,\n",
    "            machine_type=batch_predict_machine_type\n",
    "        )\n",
    "\n",
    "        # run evaluation based on prediction type and feature attribution component\n",
    "        eval_task = gcc_evaluation.ModelEvaluationClassificationOp(\n",
    "            project=project,\n",
    "            location=gcp_region,\n",
    "            root_dir=batch_prediction_output_path,\n",
    "            classification_type=\"multiclass\",\n",
    "            predictions_format=\"jsonl\",\n",
    "            predictions_gcs_source=batch_prediction_task.outputs[\"gcs_output_directory\"],\n",
    "            ground_truth_format=\"jsonl\",\n",
    "            ground_truth_gcs_source=[test_dataset_file_path],\n",
    "            target_field_name=target_column_name,\n",
    "            class_labels=class_names,\n",
    "            model=training_task.outputs[\"model\"]\n",
    "        )\n",
    "\n",
    "        # get feature attributions\n",
    "        feature_attribution_task = gcc_evaluation.ModelEvaluationFeatureAttributionOp(\n",
    "            project=project,\n",
    "            location=gcp_region,\n",
    "            root_dir=batch_prediction_output_path,\n",
    "            predictions_format=\"jsonl\",\n",
    "            predictions_gcs_source=batch_prediction_task.outputs[\"gcs_output_directory\"],\n",
    "        )\n",
    "\n",
    "        # Import the evaluation results to the model resource\n",
    "        model_import_evaluation_task = gcc_evaluation.ModelImportEvaluationOp(\n",
    "            classification_metrics=eval_task.outputs[\"evaluation_metrics\"],\n",
    "            feature_attributions=feature_attribution_task.outputs[\"feature_attributions\"],\n",
    "            model=training_task.outputs[\"model\"],\n",
    "            dataset_type=\"jsonl\",\n",
    "        )\n",
    "\n",
    "        conditional_dep_task = make_deployment_decision(\n",
    "            project=project,\n",
    "            staging_bucket=staging_bucket,\n",
    "            metric_name=deployment_metric,\n",
    "            threshold=deployment_threshold,\n",
    "        ).after(eval_task)\n",
    "\n",
    "\n",
    "        with dsl.Condition(conditional_dep_task.outputs[\"dep_decision\"] == \"true\", name=\"deploy_decision\"):\n",
    "\n",
    "            endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "                project=project,\n",
    "                location=gcp_region,\n",
    "                display_name=endpoint_display_name,\n",
    "            )\n",
    "\n",
    "            gcc_aip.ModelDeployOp(\n",
    "                model=training_task.outputs[\"model\"],\n",
    "                endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "                dedicated_resources_min_replica_count=dedicated_resources_min_replica_count,\n",
    "                dedicated_resources_max_replica_count=dedicated_resources_max_replica_count,\n",
    "                dedicated_resources_machine_type=dedicated_resources_machine_type,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6cfa7ea-f5ed-4d27-8a0a-d70276b475a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"credit_card_fraud_detection_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fd3fa08-00e9-465e-9b82-6e668b88b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"credit-card-fraud-detection-tf-pipeline-{}\".format(UUID),\n",
    "    template_path=\"credit_card_fraud_detection_pipeline.json\",\n",
    "    pipeline_root=WORKDIR,\n",
    "    parameter_values=dict(\n",
    "        project=PROJECT_ID,\n",
    "        gcp_region=\"us-central1\",\n",
    "        dataset_display_name=\"credit_card_fraud_detection\",\n",
    "        bq_source=\"bq://aadev-end-to-end-pipeline-demo.gcs_lake.credit_card_fraud_detection\",\n",
    "        training_display_name=\"credit-card-fraud-detection-tf-train\",\n",
    "        staging_bucket=WORKDIR,\n",
    "        container_uri=\"us-central1-docker.pkg.dev/aadev-end-to-end-pipeline-demo/vertex-ai-pipeline-custom-training-jobs/credit_card_fraud_detection_tf:latest\",\n",
    "        target_column_name=\"Class\",\n",
    "        class_names=[\"0\", \"1\"],\n",
    "        key_columns=[\"key\"],\n",
    "        bq_dest=os.path.join(\"bq://\", PROJECT_ID),\n",
    "        model_display_name=\"credit-card-fraud-detection-tf\",\n",
    "        model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-9:latest\",\n",
    "        training_machine_type=\"n1-standard-4\",\n",
    "        batch_predict_display_name=\"credit-card-fraud-detection-tf-batch-predict\",\n",
    "        batch_predict_machine_type=\"n1-standard-4\",\n",
    "        deployment_metric=\"auPrc\",\n",
    "        deployment_threshold=0.95,\n",
    "        aiplatform_host=\"us-central1-aiplatform.googleapis.com\",\n",
    "        endpoint_display_name=\"credit-card-fraud-detection-tf\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\"\n",
    "    ),\n",
    "    enable_caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbaabc9",
   "metadata": {},
   "source": [
    "## Submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d5f722a-a223-45e1-bf7d-6a3b5c93685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/654648321406/locations/us-central1/pipelineJobs/credit-card-fraud-detection-training-20221120104203\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/654648321406/locations/us-central1/pipelineJobs/credit-card-fraud-detection-training-20221120104203')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/credit-card-fraud-detection-training-20221120104203?project=654648321406\n"
     ]
    }
   ],
   "source": [
    "ml_pipeline_job.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
